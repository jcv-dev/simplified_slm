# -*- coding: utf-8 -*-

"""
Evaluation metrics for byte-level language models.

Includes:
- Bits per byte (BPB) - primary metric for byte-level LM
- Perplexity - per-byte perplexity
- Accuracy - next-byte prediction accuracy
- UTF-8 validity - percentage of valid UTF-8 in generated text
"""

from __future__ import annotations

import math
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader


@dataclass
class EvaluationMetrics:
    """Container for all evaluation metrics."""
    
    # Language modeling metrics
    loss: float = 0.0
    bpb: float = 0.0  # Bits per byte
    perplexity: float = 0.0
    accuracy: float = 0.0
    top5_accuracy: float = 0.0
    
    # Generation quality metrics
    utf8_validity: float = 0.0
    avg_generation_length: float = 0.0
    
    # Efficiency metrics
    tokens_per_second: float = 0.0
    memory_mb: float = 0.0
    
    # Sample counts
    num_samples: int = 0
    num_tokens: int = 0
    
    def to_dict(self) -> Dict[str, float]:
        return {
            "loss": self.loss,
            "bpb": self.bpb,
            "perplexity": self.perplexity,
            "accuracy": self.accuracy,
            "top5_accuracy": self.top5_accuracy,
            "utf8_validity": self.utf8_validity,
            "avg_generation_length": self.avg_generation_length,
            "tokens_per_second": self.tokens_per_second,
            "memory_mb": self.memory_mb,
            "num_samples": self.num_samples,
            "num_tokens": self.num_tokens,
        }
    
    def __str__(self) -> str:
        lines = [
            f"Loss: {self.loss:.4f}",
            f"BPB: {self.bpb:.4f}",
            f"Perplexity: {self.perplexity:.2f}",
            f"Accuracy: {self.accuracy * 100:.2f}%",
            f"Top-5 Accuracy: {self.top5_accuracy * 100:.2f}%",
        ]
        if self.utf8_validity > 0:
            lines.append(f"UTF-8 Validity: {self.utf8_validity * 100:.2f}%")
        if self.tokens_per_second > 0:
            lines.append(f"Throughput: {self.tokens_per_second:.1f} tokens/sec")
        return "\n".join(lines)


def compute_bpb(loss: float) -> float:
    """
    Compute bits per byte from cross-entropy loss.
    
    BPB = loss / ln(2)
    
    Args:
        loss: Cross-entropy loss (in nats)
        
    Returns:
        Bits per byte
    """
    return loss / math.log(2)


def compute_perplexity(loss: float, clamp_max: float = 100000.0) -> float:
    """
    Compute perplexity from cross-entropy loss.
    
    Perplexity = exp(loss)
    
    Args:
        loss: Cross-entropy loss (in nats)
        clamp_max: Maximum perplexity value to prevent overflow
        
    Returns:
        Perplexity
    """
    return min(math.exp(loss), clamp_max)


def compute_accuracy(
    logits: torch.Tensor,
    labels: torch.Tensor,
    ignore_index: int = -100,
) -> Tuple[float, float]:
    """
    Compute top-1 and top-5 accuracy.
    
    Args:
        logits: Model logits (B, L, V)
        labels: Ground truth labels (B, L)
        ignore_index: Label value to ignore in computation
        
    Returns:
        Tuple of (top1_accuracy, top5_accuracy)
    """
    # Flatten
    logits = logits.view(-1, logits.size(-1))
    labels = labels.view(-1)
    
    # Mask
    mask = labels != ignore_index
    if not mask.any():
        return 0.0, 0.0
    
    logits = logits[mask]
    labels = labels[mask]
    
    # Top-1
    preds = logits.argmax(dim=-1)
    top1_correct = (preds == labels).float().sum().item()
    
    # Top-5
    top5_preds = logits.topk(min(5, logits.size(-1)), dim=-1).indices
    top5_correct = (top5_preds == labels.unsqueeze(-1)).any(dim=-1).float().sum().item()
    
    total = mask.sum().item()
    
    return top1_correct / total, top5_correct / total


def compute_utf8_validity(byte_sequences: List[List[int]]) -> float:
    """
    Compute percentage of generated byte sequences that are valid UTF-8.
    
    Args:
        byte_sequences: List of byte sequences (each is list of ints 0-255)
        
    Returns:
        Fraction of valid UTF-8 sequences (0.0 to 1.0)
    """
    if not byte_sequences:
        return 0.0
    
    valid_count = 0
    for seq in byte_sequences:
        try:
            bytes(seq).decode('utf-8')
            valid_count += 1
        except (UnicodeDecodeError, ValueError):
            pass
    
    return valid_count / len(byte_sequences)


def compute_utf8_char_validity(byte_sequence: List[int]) -> float:
    """
    Compute percentage of bytes in a sequence that form valid UTF-8 characters.
    
    Args:
        byte_sequence: Single byte sequence (list of ints 0-255)
        
    Returns:
        Fraction of bytes that are part of valid UTF-8 characters
    """
    if not byte_sequence:
        return 0.0
    
    data = bytes(byte_sequence)
    valid_bytes = 0
    i = 0
    
    while i < len(data):
        # Try to decode one character
        for char_len in range(1, 5):
            if i + char_len > len(data):
                break
            try:
                data[i:i + char_len].decode('utf-8')
                valid_bytes += char_len
                i += char_len
                break
            except UnicodeDecodeError:
                continue
        else:
            # No valid character found, skip this byte
            i += 1
    
    return valid_bytes / len(data)


@torch.no_grad()
def evaluate_model(
    model: nn.Module,
    dataloader: DataLoader,
    device: str = 'cuda',
    max_batches: Optional[int] = None,
) -> EvaluationMetrics:
    """
    Evaluate a language model on a dataset.
    
    Args:
        model: The language model to evaluate
        dataloader: DataLoader with evaluation data
        device: Device to run evaluation on
        max_batches: Maximum number of batches to evaluate (for speed)
        
    Returns:
        EvaluationMetrics with all computed metrics
    """
    model.eval()
    
    total_loss = 0.0
    total_tokens = 0
    total_top1_correct = 0
    total_top5_correct = 0
    num_batches = 0
    
    for batch in dataloader:
        if max_batches and num_batches >= max_batches:
            break
        
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)
        attention_mask = batch.get('attention_mask')
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)
        
        outputs = model(
            input_ids=input_ids,
            labels=labels,
            attention_mask=attention_mask,
        )
        
        # Get loss (already averaged per token)
        batch_loss = outputs.loss.item()
        
        # Count valid tokens
        mask = labels != -100
        batch_tokens = mask.sum().item()
        
        total_loss += batch_loss * batch_tokens
        total_tokens += batch_tokens
        
        # Compute accuracy
        top1_acc, top5_acc = compute_accuracy(outputs.logits, labels)
        total_top1_correct += top1_acc * batch_tokens
        total_top5_correct += top5_acc * batch_tokens
        
        num_batches += 1
    
    # Aggregate metrics
    avg_loss = total_loss / max(total_tokens, 1)
    
    metrics = EvaluationMetrics(
        loss=avg_loss,
        bpb=compute_bpb(avg_loss),
        perplexity=compute_perplexity(avg_loss),
        accuracy=total_top1_correct / max(total_tokens, 1),
        top5_accuracy=total_top5_correct / max(total_tokens, 1),
        num_samples=num_batches * dataloader.batch_size,
        num_tokens=total_tokens,
    )
    
    return metrics


@torch.no_grad()
def evaluate_generation_quality(
    model: nn.Module,
    tokenizer,
    prompts: List[str],
    max_new_tokens: int = 100,
    device: str = 'cuda',
    **generation_kwargs,
) -> Tuple[List[str], EvaluationMetrics]:
    """
    Evaluate generation quality with multiple prompts.
    
    Args:
        model: The language model
        tokenizer: ByteTokenizer for encoding/decoding
        prompts: List of prompt strings
        max_new_tokens: Maximum tokens to generate per prompt
        device: Device to run on
        **generation_kwargs: Additional args for model.generate()
        
    Returns:
        Tuple of (generated_texts, metrics)
    """
    import time
    
    model.eval()
    generated_texts = []
    byte_sequences = []
    total_time = 0.0
    total_new_tokens = 0
    
    for prompt in prompts:
        # Encode prompt
        encoded = tokenizer.encode([prompt])
        input_data = encoded[0]['input_ids']
        # Handle both numpy arrays and tensors
        if hasattr(input_data, 'to'):
            input_ids = input_data.to(device)
        else:
            import numpy as np
            if isinstance(input_data, np.ndarray):
                input_ids = torch.from_numpy(input_data).to(device)
            else:
                input_ids = torch.tensor(input_data, device=device)
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        
        prompt_len = input_ids.size(1)
        
        # Generate
        start_time = time.time()
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            **generation_kwargs,
        )
        gen_time = time.time() - start_time
        
        # Decode
        new_tokens = output_ids[0, prompt_len:].tolist()
        byte_sequences.append(new_tokens)
        total_new_tokens += len(new_tokens)
        total_time += gen_time
        
        try:
            text = tokenizer.decode(output_ids[0].tolist())
            generated_texts.append(text)
        except Exception:
            generated_texts.append(f"[DECODE_ERROR: {new_tokens[:20]}...]")
    
    # Compute metrics
    utf8_validity = compute_utf8_validity(byte_sequences)
    avg_length = total_new_tokens / max(len(prompts), 1)
    tokens_per_second = total_new_tokens / max(total_time, 1e-6)
    
    metrics = EvaluationMetrics(
        utf8_validity=utf8_validity,
        avg_generation_length=avg_length,
        tokens_per_second=tokens_per_second,
        num_samples=len(prompts),
        num_tokens=total_new_tokens,
    )
    
    return generated_texts, metrics


def compute_model_flops(
    model: nn.Module,
    input_ids: torch.Tensor,
    verbose: bool = False,
) -> int:
    """
    Estimate FLOPs for a model forward pass.
    
    Note: This is an approximation. For accurate counts,
    use profiling tools like fvcore or torch.profiler.
    
    Args:
        model: The model to profile
        input_ids: Sample input
        verbose: Print breakdown by layer
        
    Returns:
        Estimated FLOPs
    """
    try:
        from fvcore.nn import FlopCountAnalysis
        flops = FlopCountAnalysis(model, input_ids)
        if verbose:
            print(flops.by_module())
        return flops.total()
    except ImportError:
        # Fallback: rough estimate based on parameters
        total_params = sum(p.numel() for p in model.parameters())
        seq_len = input_ids.size(-1)
        # Rough: 2 FLOPs per param per token (forward only)
        return 2 * total_params * seq_len


def compute_memory_footprint(model: nn.Module) -> Dict[str, float]:
    """
    Compute memory footprint of a model.
    
    Args:
        model: The model to measure
        
    Returns:
        Dict with memory stats in MB
    """
    total_params = 0
    total_bytes = 0
    trainable_params = 0
    
    for param in model.parameters():
        total_params += param.numel()
        total_bytes += param.numel() * param.element_size()
        if param.requires_grad:
            trainable_params += param.numel()
    
    # Buffer memory (non-parameter tensors)
    buffer_bytes = sum(
        buf.numel() * buf.element_size()
        for buf in model.buffers()
    )
    
    return {
        "total_params": total_params,
        "trainable_params": trainable_params,
        "param_memory_mb": total_bytes / (1024 ** 2),
        "buffer_memory_mb": buffer_bytes / (1024 ** 2),
        "total_memory_mb": (total_bytes + buffer_bytes) / (1024 ** 2),
    }


class MetricsTracker:
    """
    Track metrics over training for analysis.
    
    Stores history of metrics and computes running averages.
    """
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.history: Dict[str, List[float]] = {}
        self.step_history: List[int] = []
    
    def update(self, step: int, metrics: Dict[str, float]) -> None:
        """Add metrics for a step."""
        self.step_history.append(step)
        for key, value in metrics.items():
            if key not in self.history:
                self.history[key] = []
            self.history[key].append(value)
    
    def get_running_average(self, key: str) -> float:
        """Get running average over window."""
        if key not in self.history:
            return 0.0
        values = self.history[key][-self.window_size:]
        return sum(values) / max(len(values), 1)
    
    def get_history(self, key: str) -> Tuple[List[int], List[float]]:
        """Get full history for a metric."""
        if key not in self.history:
            return [], []
        return self.step_history, self.history[key]
    
    def get_latest(self, key: str) -> float:
        """Get most recent value."""
        if key not in self.history or not self.history[key]:
            return 0.0
        return self.history[key][-1]
    
    def get_best(self, key: str, mode: str = 'min') -> Tuple[int, float]:
        """Get best value and step."""
        if key not in self.history or not self.history[key]:
            return 0, float('inf') if mode == 'min' else float('-inf')
        
        values = self.history[key]
        if mode == 'min':
            best_idx = values.index(min(values))
        else:
            best_idx = values.index(max(values))
        
        return self.step_history[best_idx], values[best_idx]
    
    def summary(self) -> Dict[str, Dict[str, float]]:
        """Get summary of all tracked metrics."""
        summary = {}
        for key in self.history:
            values = self.history[key]
            summary[key] = {
                "latest": values[-1] if values else 0.0,
                "mean": sum(values) / max(len(values), 1),
                "min": min(values) if values else 0.0,
                "max": max(values) if values else 0.0,
            }
        return summary
